{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cefee2-bf17-4858-8ec5-824f5bbbf9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 1 \n",
    "\n",
    "\"The relationship between polynomial functions and kernel functions in machine learning algorithms can be explained through the concept of the polynomial kernel function. The polynomial kernel function is a type of kernel function used in machine learning, particularly in Support Vector Machines (SVMs). \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69258a1d-84af-402d-989d-2df7264a95e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 2 \n",
    "\n",
    "'by setting the kernel value to poly'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a563a-7195-423f-a46a-7fffbe8aa668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 3\n",
    "\n",
    "'''\n",
    "increasing the value ofϵ in SVR generally decreases the number of support vectors, leading to a simpler model that may underfit if ϵ is too large.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013be73e-8165-422d-ab98-528cb5f299f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 4 \n",
    "\n",
    "'''\n",
    "1. Kernel Function\n",
    "The kernel function transforms the input data into a higher-dimensional space where it may be easier to perform the regression. Common kernel functions include:\n",
    "\n",
    "\n",
    "Impact: The choice of kernel function determines the transformation applied to the data. The RBF kernel is commonly used for non-linear relationships, while the linear kernel is suitable for linear relationships.\n",
    "\n",
    "When to adjust:\n",
    "\n",
    "Linear Kernel: Use when the data has a linear relationship.\n",
    "Polynomial Kernel: Use when you expect polynomial relationships and need more flexibility than a linear kernel.\n",
    "RBF Kernel: Use when the relationship between the features and target is complex and non-linear.\n",
    "2. \n",
    "𝐶\n",
    "C Parameter\n",
    "The \n",
    "𝐶\n",
    "C parameter (regularization parameter) controls the trade-off between achieving a low error on the training data and minimizing the model complexity.\n",
    "\n",
    "Impact: A smaller \n",
    "𝐶\n",
    "C value creates a wider margin, allowing more slack (errors) but potentially better generalization. A larger \n",
    "𝐶\n",
    "C value creates a narrower margin, penalizing errors more and focusing on minimizing training error.\n",
    "When to adjust:\n",
    "\n",
    "Increase \n",
    "𝐶\n",
    "C: When the model is underfitting and you want to reduce bias by fitting the training data more closely.\n",
    "Decrease \n",
    "𝐶\n",
    "C: When the model is overfitting and you want to improve generalization by allowing more errors on the training data.\n",
    "3. \n",
    "𝜖\n",
    "ϵ Parameter\n",
    "The \n",
    "𝜖\n",
    "ϵ parameter defines the margin of tolerance where no penalty is given for errors. It's the width of the \n",
    "𝜖\n",
    "ϵ-tube around the regression line.\n",
    "\n",
    "Impact: A larger \n",
    "𝜖\n",
    "ϵ value allows for a wider margin where errors are tolerated, leading to fewer support vectors and potentially a simpler model. A smaller \n",
    "𝜖\n",
    "ϵ value results in a narrower margin, making the model more sensitive to errors.\n",
    "When to adjust:\n",
    "\n",
    "Increase \n",
    "𝜖\n",
    "ϵ: When you want a simpler model with fewer support vectors, which may help in reducing overfitting.\n",
    "Decrease \n",
    "𝜖\n",
    "ϵ: When you need a more accurate model that captures more details in the data, which may help in reducing bias.\n",
    "4. \n",
    "𝛾\n",
    "γ Parameter (for RBF and Polynomial Kernels)\n",
    "The \n",
    "𝛾\n",
    "γ parameter defines the influence of a single training example. In the RBF kernel, it determines the spread of the kernel, and in the polynomial kernel, it affects the shape of the polynomial curve.\n",
    "\n",
    "Impact: A higher \n",
    "𝛾\n",
    "γ value means that each training example has a high influence, leading to a more complex model that can capture finer details. A lower \n",
    "𝛾\n",
    "γ value results in a smoother model that captures the broader trends.\n",
    "When to adjust:\n",
    "\n",
    "Increase \n",
    "𝛾\n",
    "γ: When the model is underfitting and you need to capture more complex relationships in the data.\n",
    "Decrease \n",
    "𝛾\n",
    "γ: When the model is overfitting and you need to smooth out the model to improve generalization.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e96628f-a604-483d-adef-80f9596145cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "Precision: 0.98\n",
      "Recall: 0.98\n",
      "F1-score: 0.98\n"
     ]
    }
   ],
   "source": [
    "#question 5 \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "df = pd.DataFrame(iris.data , columns=iris.feature_names)\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "svc = svm.SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c5a14e-9afc-429c-b3da-ad53d20c15d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
