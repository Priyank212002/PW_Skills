{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a60a9-de52-44b4-867b-fcbc7ba8ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 1 \n",
    "\n",
    "\"Lasso Regression, short for Least Absolute Shrinkage and Selection Operator, is a linear regression technique that adds a penalty term to the linear regression objective function. It is primarily used for feature selection and regularization. Lasso differs from other regression techniques, such as simple linear regression and Ridge Regression, in its use of the L1 regularization term.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e3b995-a9b2-43a8-9e3b-d1f11832d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 2 \n",
    "\n",
    "\"The main advantage of using Lasso Regression for feature selection is its ability to automatically identify and select the most relevant features while setting less important features' coefficients to exactly zero. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00572b12-21b6-4825-b481-31f006b3e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 3 \n",
    "\n",
    "'''\n",
    "Non-zero Coefficients: In Lasso, some coefficients may be exactly zero, indicating that the corresponding features have been entirely excluded from the model. This is a fundamental aspect of Lasso and implies that the eliminated features are not contributing to the predictions.\n",
    "\n",
    "Non-zero Coefficients' Sign and Magnitude: For the non-zero coefficients, their sign (+ or -) indicates the direction of the relationship with the target variable. A positive coefficient suggests that an increase in the feature's value leads to an increase in the target variable, while a negative coefficient implies the opposite. The magnitude of the coefficient indicates the strength of the relationship. Larger magnitude coefficients have a more significant impact on the target variable.\n",
    "\n",
    "Relative Importance: You can compare the magnitudes of the non-zero coefficients to assess the relative importance of each feature in making predictions. Features with larger coefficients have a stronger influence on the model's output.\n",
    "\n",
    "Feature Elimination: The presence of zero coefficients indicates that the corresponding features are considered irrelevant for making predictions by the Lasso model. This feature selection aspect of Lasso helps in simplifying the model and improving its generalization by focusing on the most informative features.\n",
    "\n",
    "Interactions and Non-linearity: Lasso Regression assumes a linear relationship between features and the target variable. It may not capture complex interactions or non-linearities in the data. If these are present, additional feature engineering or considering other regression techniques may be necessary.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c384c0-51ea-458b-a2be-85b9fbbfdc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 4 \n",
    "\n",
    "'''\n",
    "Effect on Sparsity: Increasing the value of alpha makes Lasso more stringent in penalizing the absolute values of the coefficients. As alpha becomes larger, the model is more likely to set coefficients to exactly zero, leading to a sparser model with fewer non-zero coefficients. This helps with feature selection by automatically excluding irrelevant features.\n",
    "\n",
    "Effect on Model Complexity: A smaller alpha value allows the model to retain more features with non-zero coefficients, leading to a more complex model. Conversely, a larger alpha simplifies the model by reducing the number of features with non-zero coefficients.\n",
    "\n",
    "Trade-off with Fit: Adjusting alpha is a trade-off between model fit and model simplicity. Smaller alpha values lead to models that fit the training data more closely but are more prone to overfitting, while larger alpha values result in simpler models that may have slightly reduced predictive performance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e2f4cc-4fcc-4ee9-9aac-a26a165d206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 5 \n",
    "\n",
    "\"while Lasso Regression is fundamentally a linear regression technique, you can adapt it to address non-linear regression problems by incorporating non-linear transformations, interaction terms, kernel tricks, splines, or by considering more advanced models. The choice of method depends on the nature of your data and the specific non-linear relationships you need to capture.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71fd8d0-524c-49d4-88db-c69077713631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 6 \n",
    "\n",
    "\"Ridge and Lasso Regression are two regularization techniques with distinct behaviors regarding feature selection and model complexity. Ridge tends to reduce coefficients' magnitude but retains all features, while Lasso encourages feature selection by setting some coefficients to zero, producing a sparser and more interpretable model. The choice between Ridge and Lasso depends on your specific data and modeling goals, with Ridge often preferred for reducing multicollinearity, and Lasso for feature selection in high-dimensional datasets.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db08f4a-6312-4761-abe3-138a4d29b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 7 \n",
    "\n",
    "'''\n",
    "\n",
    "Feature Selection: Lasso Regression encourages feature selection by setting some of the coefficients to exactly zero. When there is multicollinearity, Lasso tends to select one variable from a group of highly correlated variables and sets the coefficients of the others to zero. This effectively eliminates the redundant features and retains only the most informative ones, which helps in reducing the multicollinearity issue.\n",
    "\n",
    "Reducing Model Complexity: By excluding some features through feature selection, Lasso simplifies the model. This reduction in the number of features can lead to a more stable and interpretable model, which is less prone to multicollinearity-related problems.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475c3ead-8355-4171-bf6f-d6468c3dc261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 8 \n",
    "\n",
    "'''\n",
    "Create a Range of Lambda Values: Start by defining a range of lambda values that you want to test. It's common to use a logarithmic scale, spanning several orders of magnitude. For example, you can start with small values like 0.001 and increase in increments to larger values like 100.\n",
    "\n",
    "Split the Data: Divide your dataset into training and validation (or test) sets. The training set is used to train the Lasso Regression model, and the validation set is used to evaluate its performance for different lambda values.\n",
    "\n",
    "Cross-Validation: Perform k-fold cross-validation (often 5 or 10 folds) on the training data. In each fold, you train the Lasso Regression model on k-1 subsets of the training data and validate it on the remaining subset. This process is repeated for each lambda value.\n",
    "\n",
    "Model Training: For each lambda value, fit a Lasso Regression model to the training data in each fold. The lambda value serves as the regularization strength.\n",
    "\n",
    "Model Evaluation: Calculate a performance metric (e.g., mean squared error, mean absolute error, or others) on the validation set for each fold and each lambda value.\n",
    "\n",
    "Average Performance: Compute the average performance metric across all k folds for each lambda value. This provides a more stable estimate of the model's performance for different regularization strengths.\n",
    "\n",
    "Select the Optimal Lambda: Choose the lambda value that corresponds to the best (smallest) average performance metric. This lambda value is considered the optimal one for your Lasso Regression model.\n",
    "\n",
    "Model Training on Full Dataset: After selecting the optimal lambda, train the Lasso Regression model on the entire training dataset using that lambda.\n",
    "\n",
    "Model Evaluation: Finally, assess the model's performance on a separate test dataset to ensure it generalizes well to unseen data.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
