{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca8b77d-1995-4858-b7f3-32632a607eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 1 \n",
    "\n",
    "\"simple linear regression is used for one independent variable, while multiple linear regression is employed when there are multiple independent variables, allowing for a more comprehensive analysis of their combined impact on the dependent variable.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d20aa58-be9f-495a-abc2-924f34c8578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 2  \n",
    "\n",
    "'''\n",
    "Linear regression relies on several key assumptions, and it's important to check whether these assumptions hold in a given dataset to ensure the validity of your regression analysis. Here are the main assumptions of linear regression and ways to check them:\n",
    "\n",
    "Linearity: This assumption assumes that the relationship between the independent variables and the dependent variable is linear. You can check this by creating scatterplots of the variables and looking for a roughly linear pattern. If the points seem to follow a straight line, the linearity assumption is more likely to hold.\n",
    "\n",
    "Independence: The observations should be independent of each other, meaning that the value of the dependent variable for one data point should not be influenced by another. You can check this by examining the data collection process and ensuring there is no serial correlation or auto-correlation in the residuals (errors).\n",
    "\n",
    "Homoscedasticity: This assumption assumes that the variance of the residuals (the differences between observed and predicted values) is constant across all levels of the independent variables. To check for this, you can create a scatterplot of the residuals against the predicted values and look for a consistent spread of points. A funnel-shaped pattern or any systematic change in spread may indicate heteroscedasticity.\n",
    "\n",
    "Normality of Residuals: The residuals should follow a normal distribution, meaning they are symmetrically distributed around zero. You can assess this assumption through a normal probability plot or a histogram of the residuals. If the residuals deviate significantly from normality, it might be necessary to consider transformations or use robust regression techniques.\n",
    "\n",
    "No or Little Multicollinearity: Multicollinearity occurs when independent variables in a multiple regression model are highly correlated, which can make it challenging to interpret the individual effects of each variable. You can check for multicollinearity using correlation matrices or variance inflation factors (VIF). If VIF values are significantly above 1, it suggests multicollinearity.\n",
    "\n",
    "\n",
    "To assess these assumptions, you can also perform diagnostic tests such as the Durbin-Watson test for autocorrelation, the Breusch-Pagan test for heteroscedasticity, and the Anderson-Darling or Shapiro-Wilk tests for normality of residuals. Additionally, visually inspecting residual plots and Q-Q plots can be helpful in identifying violations of these assumptions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1d093-d077-4dd3-bd93-678037f80d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 3 \n",
    "\n",
    "\"Interpreting the slope and intercept allows you to understand the baseline value and the rate of change in the dependent variable in response to changes in the independent variable, which is essential for making predictions and drawing insights from your regression analysis.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd7c240-8495-40b6-8b9b-8c2a9916ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 4 \n",
    "\n",
    "\"Gradient descent is a fundamental optimization algorithm used in machine learning and various other fields to minimize a cost or loss function. Its primary purpose is to find the minimum of a function by iteratively adjusting the model's parameters (weights) in the direction of steepest descent (negative gradient) of the function. In the context of machine learning, gradient descent is commonly used to update the parameters of a model to minimize the error between the predicted and actual values.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f2ace-ed79-4412-a07c-a8824335432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 5 \n",
    "\n",
    "\"The main difference between multiple linear regression and simple linear regression is the number of independent variables involved. In simple linear regression, there is only one independent variable, and it aims to establish a linear relationship between that single variable and the dependent variable. In contrast, multiple linear regression incorporates two or more independent variables, allowing for a more comprehensive analysis of how these variables collectively influence the dependent variable. Multiple linear regression provides a more realistic representation of complex real-world relationships, making it a valuable tool in many fields, such as economics, social sciences, and natural sciences, where multiple factors often come into play when explaining or predicting outcomes.\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafb0397-8acf-4a6b-9625-f513740d497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 6 \n",
    "\n",
    "\"Multicollinearity is a common issue that can arise in multiple linear regression when two or more independent variables in the model are highly correlated with each other. In other words, it occurs when the independent variables are not truly independent but rather exhibit strong linear relationships among themselves. Multicollinearity can complicate the interpretation of the model and lead to unreliable estimates of the coefficients of the independent variables. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d85dd-dcc7-4c6a-a39b-30c01bb24412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 7 \n",
    "\n",
    "\"Polynomial regression is a variation of linear regression used to model relationships between a dependent variable and one or more independent variables. What sets polynomial regression apart from simple linear regression is its ability to capture nonlinear patterns in the data by introducing polynomial terms. In a polynomial regression model, the relationship between the independent and dependent variables is represented as a polynomial equation of a specified degree. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e16633-2db2-406a-9104-a5c1d8cf8386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 8 \n",
    "\n",
    "\"Polynomial regression is preferred in situations where it's clear that the relationship between the independent and dependent variables is not linear and cannot be adequately represented by a straight line. Common use cases include modeling natural phenomena, biological growth, economic data, and various scientific and engineering applications. However, it's crucial to be cautious with the choice of polynomial degree and regularly employ techniques like cross-validation to guard against overfitting. In general, the decision to use polynomial regression should be driven by the specific characteristics of the data and the objectives of the analysis.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
